Thoughtcapable artificial beings appeared as storytelling devices in antiquity and have been common in fiction as in Mary Shelleys Frankenstein  Rossums Universal Robots These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence

The study of mechanical or formal reasoning began with philosophers and mathematicians in antiquity The study of mathematical logic led directly to Alan Turings theory of computation which suggested that a machine by shuffling symbols as simple as  and  could simulate any conceivable act of mathematical deduction This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis Along with concurrent discoveries in neurobiology information theory and cybernetics this led researchers to consider the possibility of building an electronic brain Turing proposed changing the question from whether a machine was intelligent to whether or not it is possible for machinery to show intelligent behaviour The first work that is now generally recognized as AI was McCullouch and Pitts  formal design for Turingcomplete artificial neurons

The field of AI research was born at a workshop at Dartmouth College in  where the term Artificial Intelligence was coined by John McCarthy to distinguish the field from cybernetics and escape the influence of the cyberneticist Norbert Wiener Attendees Allen Newell CMU Herbert Simon CMU John McCarthy MIT Marvin Minsky MIT and Arthur Samuel IBM became the founders and leaders of AI research They and their students produced programs that the press described as astonishing: computers were learning checkers strategies c  and by  were reportedly playing better than the average human solving word problems in algebra proving logical theorems Logic Theorist first run c  and speaking English By the middle of the s research in the US was heavily funded by the Department of Defense and laboratories had been established around the world AIs founders were optimistic about the future: Herbert Simon predicted machines will be capable within twenty years of doing any work a man can do Marvin Minsky agreed writing within a generation  the problem of creating artificial intelligence will substantially be solved

They failed to recognize the difficulty of some of the remaining tasks Progress slowed and in  in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects both the US and British governments cut off exploratory research in AI The next few years would later be called an AI winter a period when obtaining funding for AI projects was difficult

In the early s AI research was revived by the commercial success of expert systems a form of AI program that simulated the knowledge and analytical skills of human experts By  the market for AI had reached over a billion dollars At the same time Japans fifth generation computer project inspired the US and British governments to restore funding for academic research However beginning with the collapse of the Lisp Machine market in  AI once again fell into disrepute and a second longerlasting hiatus began

The development of metal–oxide–semiconductor MOS verylargescale integration VLSI in the form of complementary MOS CMOS transistor technology enabled the development of practical artificial neural network ANN technology in the s A landmark publication in the field was the  book Analog VLSI Implementation of Neural Systems by Carver A Mead and Mohammed Ismail

In the late s and early st century AI began to be used for logistics data mining medical diagnosis and other areas The success was due to increasing computational power see Moores law and transistor count greater emphasis on solving specific problems new ties between AI and other fields such as statistics economics and mathematics and a commitment by researchers to mathematical methods and scientific standards Deep Blue became the first computer chessplaying system to beat a reigning world chess champion Garry Kasparov on  May 

In  a Jeopardy quiz show exhibition match IBMs question answering system Watson defeated the two greatest Jeopardy champions Brad Rutter and Ken Jennings by a significant margin Faster computers algorithmic improvements and access to large amounts of data enabled advances in machine learning and perception; datahungry deep learning methods started to dominate accuracy benchmarks around  The Kinect which provides a D body–motion interface for the Xbox  and the Xbox One uses algorithms that emerged from lengthy AI research as do intelligent personal assistants in smartphones In March  AlphaGo won  out of  games of Go in a match with Go champion Lee Sedol becoming the first computer Goplaying system to beat a professional Go player without handicaps In the  Future of Go Summit AlphaGo won a threegame match with Ke Jie who at the time continuously held the world No  ranking for two years This marked the completion of a significant milestone in the development of Artificial Intelligence as Go is a relatively complex game more so than Chess

According to Bloombergs Jack Clark  was a landmark year for artificial intelligence with the number of software projects that use AI Google increased from a sporadic usage in  to more than  projects Clark also presents factual data indicating the improvements of AI since  supported by lower error rates in image processing tasks He attributes this to an increase in affordable neural networks due to a rise in cloud computing infrastructure and to an increase in research tools and datasets Other cited examples include Microsofts development of a Skype system that can automatically translate from one language to another and Facebooks system that can describe images to blind people In a  survey one in five companies reported they had incorporated AI in some offerings or processes Around  China greatly accelerated its government funding; given its large supply of data and its rapidly increasing research output some observers believe it may be on track to becoming an AI superpower However it has been acknowledged that reports regarding artificial intelligence have tended to be exaggerated

A typical AI analyzes its environment and takes actions that maximize its chance of success An AIs intended utility function or goal can be simple  if the AI wins a game of Go  otherwise or complex Do mathematically similar actions to the ones succeeded in the past Goals can be explicitly defined or induced If the AI is programmed for reinforcement learning goals can be implicitly induced by rewarding some types of behavior or punishing othersa Alternatively an evolutionary system can induce goals by using a fitness function to mutate and preferentially replicate highscoring AI systems similar to how animals evolved to innately desire certain goals such as finding food Some AI systems such as nearestneighbor instead of reason by analogy these systems are not generally given goals except to the degree that goals are implicit in their training data Such systems can still be benchmarked if the nongoal system is framed as a system whose goal is to successfully accomplish its narrow classification task

AI often revolves around the use of algorithms An algorithm is a set of unambiguous instructions that a mechanical computer can executeb A complex algorithm is often built on top of other simpler algorithms A simple example of an algorithm is the following optimal for first player recipe for play at tictactoe:

If someone has a threat that is two in a row take the remaining square Otherwise
if a move forks to create two threats at once play that move Otherwise
take the center square if it is free Otherwise
if your opponent has played in a corner take the opposite corner Otherwise
take an empty corner if one exists Otherwise
take any empty square
Many AI algorithms are capable of learning from data; they can enhance themselves by learning new heuristics strategies or rules of thumb that have worked well in the past or can themselves write other algorithms Some of the learners described below including Bayesian networks decision trees and nearestneighbor could theoretically given infinite data time and memory learn to approximate any function including which combination of mathematical functions would best describe the worldcitation needed These learners could therefore derive all possible knowledge by considering every possible hypothesis and matching them against the data In practice it is almost never possible to consider every possibility because of the phenomenon of combinatorial explosion where the amount of time needed to solve a problem grows exponentially Much of AI research involves figuring out how to identify and avoid considering broad range of possibilities that are unlikely to be beneficial For example when viewing a map and looking for the shortest driving route from Denver to New York in the East one can in most cases skip looking at any path through San Francisco or other areas far to the West; thus an AI wielding a pathfinding algorithm like A* can avoid the combinatorial explosion that would ensue if every possible route had to be ponderously considered in turn

The earliest and easiest to understand approach to AI was symbolism such as formal logic: If an otherwise healthy adult has a fever then they may have influenza A second more general approach is Bayesian inference: If the current patient has a fever adjust the probability they have influenza in suchandsuch way The third major approach extremely popular in routine business AI applications are analogizers such as SVM and nearestneighbor: After examining the records of known past patients whose temperature symptoms age and other factors mostly match the current patient X of those patients turned out to have influenza A fourth approach is harder to intuitively understand but is inspired by how the brains machinery works: the artificial neural network approach uses artificial neurons that can learn by comparing itself to the desired output and altering the strengths of the connections between its internal neurons to reinforce connections that seemed to be useful These four main approaches can overlap with each other and with evolutionary systems; for example neural nets can learn to make inferences to generalize and to make analogies Some systems implicitly or explicitly use multiple of these approaches alongside many other AI and nonAI algorithms; the best approach is often different depending on the problem

Learning algorithms work on the basis that strategies algorithms and inferences that worked well in the past are likely to continue working well in the future These inferences can be obvious such as since the sun rose every morning for the last  days it will probably rise tomorrow morning as well They can be nuanced such as X of families have geographically separate species with color variants so there is a Y chance that undiscovered black swans exist Learners also work on the basis of Occams razor: The simplest theory that explains the data is the likeliest Therefore according to Occams razor principle a learner must be designed such that it prefers simpler theories to complex theories except in cases where the complex theory is proven substantially better


The blue line could be an example of overfitting a linear function due to random noise
Settling on a bad overly complex theory gerrymandered to fit all the past training data is known as overfitting Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is Besides classic overfitting learners can also disappoint by learning the wrong lesson A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses A realworld example is that unlike humans current image classifiers dont determine the spatial relationship between components of the picture; instead they learn abstract patterns of pixels that humans are oblivious to but that linearly correlate with images of certain types of real objects Faintly superimposing such a pattern on a legitimate image results in an adversarial image that the system misclassifiesc


A selfdriving car system may use a neural network to determine which parts of the picture seem to match previous training images of pedestrians and then model those areas as slowmoving but somewhat unpredictable rectangular prisms that must be avoided
Compared with humans existing AI lacks several features of human commonsense reasoning; most notably humans have powerful mechanisms for reasoning about naïve physics such as space time and physical interactions This enables even young children to easily make inferences like If I roll this pen off a table it will fall on the floor Humans also have a powerful mechanism of folk psychology that helps them to interpret naturallanguage sentences such as The city councilmen refused the demonstrators a permit because they advocated violence A generic AI has difficulty discerning whether the ones alleged to be advocating violence are the councilmen or the demonstrators This lack of common knowledge means that AI often makes different mistakes than humans make in ways that can seem incomprehensible For example existing selfdriving cars cannot reason about the location nor the intentions of pedestrians in the exact way that humans do and instead must use nonhuman modes of reasoning to avoid accidents
